{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6. Feature Engineering.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYt98qats07rWqA14KY6IZ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vyGEr96rIkjm"},"source":["## Feature Engineering\n","\n","In this notebook, we will engineer the following features using the preprocessed data in <I>preprocessed</I> folder.\n","1. Credit Annuity Ratio\n","2. Credit Goods Price Ratio\n","3. Value of TARGET column for nearest 500 neighbours\n","4. Debt Credit Ratio\n","5. Mean of Days Credit"]},{"cell_type":"code","metadata":{"id":"prpvpEj96dvy","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1601879008528,"user_tz":-330,"elapsed":57502,"user":{"displayName":"Himanshu Bhatnagar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNnEEs3Vpa6DcPA5XsADQsENAWaVpGXrIB3zI=s64","userId":"16784833160241300445"}},"outputId":"73395d31-3379-4444-d0d4-6e234f3ea08b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CCUlX7JGgcW1"},"source":["# project directory\n","current_dir = 'Home Credit_Kaggle'\n","\n","# set the project folder as current working directory\n","import os\n","complete_path = os.path.join('/content/drive/My Drive/Colab Notebooks/',current_dir)\n","os.chdir(complete_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHDH-W3I7v1m"},"source":["import numpy as np\n","import pandas as pd\n","import time\n","from scipy.sparse import csr_matrix,save_npz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2JP6u_tzL2e"},"source":["###Load Preprocessed Data and Column Index Dictionaries"]},{"cell_type":"code","metadata":{"id":"OEfnf_W95skP","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1601879021506,"user_tz":-330,"elapsed":9098,"user":{"displayName":"Himanshu Bhatnagar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNnEEs3Vpa6DcPA5XsADQsENAWaVpGXrIB3zI=s64","userId":"16784833160241300445"}},"outputId":"86f1f7fe-6882-4aa6-90fa-c5c484dc5a92"},"source":["# load preprocessed data of files\n","\n","from scipy.sparse import load_npz\n","\n","# Application Train table\n","app_train_keys = pd.read_csv('preprocessed/app_train_keys.csv')\n","app_train_numeric_data = np.load('preprocessed/app_train_numeric_data.npy')\n","app_train_categ_data = load_npz('preprocessed/app_train_categ_data_csr.npz').todense()\n","print(app_train_keys.shape)\n","print(app_train_numeric_data.shape)\n","print(app_train_categ_data.shape)\n","print('='*120)\n","\n","# Bureau table\n","bureau_keys = pd.read_csv('preprocessed/bureau_keys.csv')\n","bureau_numeric_data = np.load('preprocessed/bureau_numeric_data.npy')\n","bureau_categ_data = load_npz('preprocessed/bureau_categ_data_csr.npz').todense()\n","print(bureau_keys.shape)\n","print(bureau_numeric_data.shape)\n","print(bureau_categ_data.shape)\n","print('='*120)\n","\n","# load dictionaries\n","import pickle\n","\n","app_train_col_index_file = open('preprocessors/app_train_col_index','rb')\n","app_train_col_index = pickle.load(app_train_col_index_file)\n","app_train_col_index_file.close()\n","\n","bureau_col_index_file = open('preprocessors/bureau_col_index','rb')\n","bureau_col_index = pickle.load(bureau_col_index_file)\n","bureau_col_index_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(307511, 2)\n","(307511, 27)\n","(307511, 188)\n","========================================================================================================================\n","(1716428, 2)\n","(1716428, 10)\n","(1716428, 23)\n","========================================================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rtmYD2nfBEau"},"source":["###Credit Annuity Ratio and Credit Goods Price Ratio"]},{"cell_type":"code","metadata":{"id":"fHT1-KEUM6v8"},"source":["# first ratio can be calculated directly by dividing AMT_CREDIT column by AMT_ANNUITY column of numeric data of input1\n","# second ratio can be calculated directly by dividing AMT_CREDIT column by AMT_GOODS_PRICE column of numeric data of input1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pkn49BoiAkCq"},"source":["###Train model for TARGET Mean of 500 nearest neighbours and Credit Annuity Ratio and Credit Goods Price Ratio"]},{"cell_type":"code","metadata":{"id":"5GhguUSIQTYW"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.impute import SimpleImputer\n","\n","def fetch_ind_app_train_cols(col_name):\n","  # function to fetch col_index of app_train columns\n","  field_key = 'application_train.csv' + col_name\n","  return app_train_col_index[field_key]\n","\n","def target_mean_500_and_ratios():\n","\n","  # calculate or fetch all features and fit the knn model  \n","\n","  # Feature 1 and 2 : credit_annuity_ratio and credit_goods_price_ratio\n","  # These features can be calculated for all the loan ids in one go\n","\n","  # fetch column values from numeric data corresponding to all loan id\n","  ind = fetch_ind_app_train_cols('AMT_CREDIT') # fetch column index\n","  amt_credit = app_train_numeric_data[:,ind]\n","  \n","  ind = fetch_ind_app_train_cols('AMT_ANNUITY') # fetch column index \n","  amt_annuity = app_train_numeric_data[:,ind]\n","  \n","  ind = fetch_ind_app_train_cols('AMT_GOODS_PRICE') # fetch column index \n","  amt_goods_price = app_train_numeric_data[:,ind]\n","\n","  credit_annuity_ratio = (amt_credit/amt_annuity).reshape(-1,1)\n","  credit_goods_price_ratio = (amt_credit/amt_goods_price).reshape(-1,1)\n","\n","  #credit_annuity_ratio = np.nan_to_num(credit_annuity_ratio)\n","  #credit_goods_price_ratio = np.nan_to_num(credit_goods_price_ratio)\n","\n","  # features for KNN\n","  # need only ext source 2 and ext source 3\n","  # credit annuity ratio has been calculated above\n","  ind = fetch_ind_app_train_cols('EXT_SOURCE_2') # fetch column index   \n","  ext_source_2 = app_train_numeric_data[:,ind].reshape(-1,1)\n","\n","  ind = fetch_ind_app_train_cols('EXT_SOURCE_3') # fetch column index     \n","  ext_source_3 = app_train_numeric_data[:,ind].reshape(-1,1)\n","\n","  # feature set\n","  x_train_knn = np.hstack((ext_source_2,ext_source_3,credit_annuity_ratio))\n","  y_train_knn = app_train_keys['TARGET']\n","\n","  # fit the model\n","  knn_model = KNeighborsClassifier(n_neighbors=500)\n","  knn_model.fit(x_train_knn,y_train_knn)\n","\n","  # return the features, train dataset for knn and model\n","  return credit_annuity_ratio,credit_goods_price_ratio,x_train_knn,knn_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSBBdTHgA0nV"},"source":["###Debt Credit Ratio and Days Credit Mean"]},{"cell_type":"code","metadata":{"id":"Fp4VIJRcYtxY"},"source":["def fetch_ind_bureau_cols(col_name):\n","  # function to fetch col_index of app_train columns\n","  field_key = 'bureau.csv' + col_name\n","  return bureau_col_index[field_key]\n","\n","def debt_credit_ratio_days_credit_mean(sk_id_curr):\n","\n","  # fetch the list of bureau ids for this sk_id_curr\n","  selected_recs = bureau_keys[bureau_keys['SK_ID_CURR'] == sk_id_curr]\n","  selected_inds = selected_recs.index # indices of the selected records\n","  count_sel_recs = len(selected_inds) # no of selected records\n","  #print(count_sel_recs)\n","\n","  debt_credit_ratio = 0\n","  days_credit_mean = 0\n","\n","  if count_sel_recs > 0: # atleast one record selected\n","\n","    # calculate starting and ending indices\n","    s_ind = selected_inds[0] # first index\n","    e_ind = selected_inds[-1] # last index\n","\n","    # store sum of amount credit sum for all bureau records\n","    ind = fetch_ind_bureau_cols('AMT_CREDIT_SUM') - 1 # fetch column index     \n","    amt_credit_sum_sum = np.sum(bureau_numeric_data[s_ind:e_ind+1,ind])\n","    \n","    # store sum of amount credit sum debt for all bureau records    \n","    ind = fetch_ind_bureau_cols('AMT_CREDIT_SUM_DEBT') - 1 # fetch column index         \n","    amt_credit_sum_debt_sum = np.sum(bureau_numeric_data[s_ind:e_ind+1,ind])\n","\n","    if amt_credit_sum_sum != 0:\n","      debt_credit_ratio = amt_credit_sum_debt_sum/amt_credit_sum_sum\n","    \n","    # find mean of days_credit field\n","    days_credit_mean = np.mean(bureau_numeric_data[s_ind:e_ind+1,0])\n","\n","  return (debt_credit_ratio,days_credit_mean)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nlxsrs_pny1C"},"source":["##Code to call above functions"]},{"cell_type":"code","metadata":{"id":"h2iHKdr2n3F1","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1601881600219,"user_tz":-330,"elapsed":2435904,"user":{"displayName":"Himanshu Bhatnagar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtNnEEs3Vpa6DcPA5XsADQsENAWaVpGXrIB3zI=s64","userId":"16784833160241300445"}},"outputId":"2f4bd492-b477-4ada-e6e1-6d62910c62f4"},"source":["def conv_3D_to_2D(array_3D):\n","  # to convert 3D array of shape (batch_size,rows,columns)\n","  # to 2D array of shape (batch_size*rows,columns)\n","  batch_size = int(array_3D.shape[0])\n","  rows = int(array_3D.shape[1])\n","  cols = int(array_3D.shape[2])\n","  return array_3D.reshape(batch_size*rows,cols)\n","##==========end of conv_3D_to_2D===========##\n","\n","# start time\n","s = time.time()\n","# start time for batch\n","s1 = time.time()\n","\n","# call below function to\n","# 1 calculate credit annuity ratio and credit goods_ratio\n","# 2 get train data set for knn and knn model itself, to calculate TARGET mean of 500 neigbors\n","credit_annuity_ratio,credit_goods_price_ratio,x_train_knn,knn_model_500 = target_mean_500_and_ratios()\n","# save knn model for future use (for test data)\n","import pickle\n","f = open('preprocessors/knn_model_500','wb')\n","pickle.dump(knn_model_500,f)\n","f.close()\n","\n","# data corresponding to engineered features\n","input8_values = np.array([[]]) # final size should be (batches*rec_count) X 1 X no of engineered features\n","\n","batches = 7\n","# for every batch\n","for b in range(batches):\n","\n","  # subset the data into batch\n","  batch_no = b + 1\n","  rec_count = 45000\n","  s_row = (batch_no - 1) * rec_count\n","  if batch_no != batches: \n","    e_row = batch_no * rec_count\n","  else:\n","    e_row = len(app_train_keys)\n","                \n","  app_train_keys_batch = app_train_keys[s_row:e_row]\n","\n","  # for every SK_ID_CURR in app_train\n","  for i,r in app_train_keys_batch.iterrows():\n","    \n","    xi_id = r['SK_ID_CURR'] #loan ID\n","      \n","    # initialize a blank row\n","    inp8 = np.array([])\n","\n","    # calculate engineered features and append to array\n","\n","    # Feature 1 and 2 : credit_annuity_ratio and credit_goods_price_ratio\n","    inp8 = np.append(inp8, credit_annuity_ratio[i])  \n","    inp8 = np.append(inp8, credit_goods_price_ratio[i])  \n","\n","    # Feature 3 and 4 : debt_credit_ratio and days_credit_mean\n","    debt_credit_ratio, days_credit_mean = debt_credit_ratio_days_credit_mean(xi_id)\n","    inp8 = np.append(inp8, float(debt_credit_ratio))\n","    inp8 = np.append(inp8, float(days_credit_mean))\n","\n","    # Feature 5 : TARGET mean of 500 nearest neighbors\n","    # use model to fetch the mean target value\n","    # use [0,1] to fetch probability corresponding to only class 1\n","    target_mean_500 = knn_model_500.predict_proba(x_train_knn[i].reshape(1,-1))[0,1]\n","    target_mean_500 = float(target_mean_500)\n","    inp8 = np.append(inp8, target_mean_500)\n","\n","    # reshape the row_arr\n","    # and expand dim to make 1 X 1 X row_size\n","    inp8 = inp8.reshape(1,-1)\n","    inp8 = np.expand_dims(inp8, axis=0)\n","\n","    # append the above arrays to final data\n","    if i == 0: # first point\n","      # input 8\n","      input8_values = inp8\n","\n","    else:\n","      # input 8\n","      input8_values = np.append(input8_values, inp8, axis=0)\n","    \n","    #if i == 100:\n","    #  break\n","\n","    # for every 15000 records processed\n","    # print time taken for 15000 records\n","    # and cumulative time taken\n","    if (i - s_row + 1) % 15000 == 0:\n","      print(\"{} records processed\".format(i - s_row + 1))\n","      print(\"Time Taken (In seconds) : \", (time.time() - s1))\n","      s1 = time.time()\n","      print(\"Total Time Taken (In seconds) : \", (time.time() - s))\n","      print('='*120)\n","      #break\n","\n","    # for given batch\n","    # save the data\n","    if (i + 1) == e_row:\n","      np.save(\"final_data/batch\"+str(batch_no)+\"/input8_values\",input8_values)\n","      print('='*5,'Data Saved for ' + str(e_row - s_row) + ' records','='*5)      \n","\n","  #===========end of inner for loop=================#\n","\n","  print('*'*120)\n","  print(\"Batch no \",str(b+1),\" completed\")\n","  print(\"Total Time Taken (In seconds) : \", (time.time() - s))\n","  s1 = time.time() # reset time for subset row count\n","  print('*'*120)\n","\n","#===========end of for loop for batches===================#\n","\n","print(input8_values.shape)\n","#print(input8_values)    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["15000 records processed\n","Time Taken (In seconds) :  102.19401288032532\n","Total Time Taken (In seconds) :  102.19408774375916\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  101.04014134407043\n","Total Time Taken (In seconds) :  203.23488545417786\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  111.40992665290833\n","Total Time Taken (In seconds) :  314.644846200943\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  1  completed\n","Total Time Taken (In seconds) :  314.9170277118683\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  107.77772903442383\n","Total Time Taken (In seconds) :  422.69528126716614\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  109.02164673805237\n","Total Time Taken (In seconds) :  531.7169797420502\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  112.40810513496399\n","Total Time Taken (In seconds) :  644.1255888938904\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  2  completed\n","Total Time Taken (In seconds) :  644.3784472942352\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  105.94949913024902\n","Total Time Taken (In seconds) :  750.3280317783356\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  107.34566187858582\n","Total Time Taken (In seconds) :  857.6737308502197\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  107.5269124507904\n","Total Time Taken (In seconds) :  965.2006776332855\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  3  completed\n","Total Time Taken (In seconds) :  965.4604959487915\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  113.2312479019165\n","Total Time Taken (In seconds) :  1078.6918377876282\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  115.58424496650696\n","Total Time Taken (In seconds) :  1194.2761170864105\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  117.91288924217224\n","Total Time Taken (In seconds) :  1312.1896419525146\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  4  completed\n","Total Time Taken (In seconds) :  1312.5037610530853\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  122.70690417289734\n","Total Time Taken (In seconds) :  1435.210773229599\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  128.48361730575562\n","Total Time Taken (In seconds) :  1563.6944279670715\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  127.4900290966034\n","Total Time Taken (In seconds) :  1691.1845047473907\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  5  completed\n","Total Time Taken (In seconds) :  1691.4426374435425\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  131.37776732444763\n","Total Time Taken (In seconds) :  1822.8204970359802\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  130.45040225982666\n","Total Time Taken (In seconds) :  1953.271229505539\n","========================================================================================================================\n","45000 records processed\n","Time Taken (In seconds) :  133.7367651462555\n","Total Time Taken (In seconds) :  2087.008724451065\n","========================================================================================================================\n","===== Data Saved for 45000 records =====\n","************************************************************************************************************************\n","Batch no  6  completed\n","Total Time Taken (In seconds) :  2087.275237798691\n","************************************************************************************************************************\n","15000 records processed\n","Time Taken (In seconds) :  137.65222787857056\n","Total Time Taken (In seconds) :  2224.927728176117\n","========================================================================================================================\n","30000 records processed\n","Time Taken (In seconds) :  139.1094069480896\n","Total Time Taken (In seconds) :  2364.0378251075745\n","========================================================================================================================\n","===== Data Saved for 37511 records =====\n","************************************************************************************************************************\n","Batch no  7  completed\n","Total Time Taken (In seconds) :  2434.3893806934357\n","************************************************************************************************************************\n","(307511, 1, 5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-BhbbJsrUBz"},"source":["# min max scale the engineered features\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# dictionary to hold all the scalers\n","eng_feat_preprocessors = {}\n","\n","# scale credit_annuity_ratio \n","# init values, scaler and transfore\n","field_scaler = MinMaxScaler(feature_range=(1e-3, 1))  \n","inp_array = input8_values[:,:,0].reshape(-1,1)\n","inp_array_scaled = field_scaler.fit_transform(inp_array)\n","# store scaler\n","eng_feat_preprocessors.update({'CREDIT_ANNUITY_RATIO':field_scaler})\n","# resize and append to final array\n","inp_array_scaled = np.expand_dims(inp_array_scaled, axis=2)\n","input8_values_scaled = inp_array_scaled # first value\n","\n","# scale credit_goods_price_ratio\n","field_scaler = MinMaxScaler(feature_range=(1e-3, 1))  \n","inp_array = input8_values[:,:,1].reshape(-1,1)\n","inp_array_scaled = field_scaler.fit_transform(inp_array)\n","eng_feat_preprocessors.update({'CREDIT_GOODS_PRICE_RATIO':field_scaler})\n","inp_array_scaled = np.expand_dims(inp_array_scaled, axis=2)\n","input8_values_scaled = np.append(input8_values_scaled,inp_array_scaled,axis = 2)\n","\n","# scale debt_credit_ratio\n","field_scaler = MinMaxScaler(feature_range=(1e-3, 1))  \n","inp_array = input8_values[:,:,2].reshape(-1,1)\n","inp_array_scaled = field_scaler.fit_transform(inp_array)\n","eng_feat_preprocessors.update({'DEBT_CREDIT_RATIO':field_scaler})\n","inp_array_scaled = np.expand_dims(inp_array_scaled, axis=2)\n","input8_values_scaled = np.append(input8_values_scaled,inp_array_scaled,axis=2)\n","\n","# scale days_credit_mean\n","field_scaler = MinMaxScaler(feature_range=(1e-3, 1))  \n","inp_array = input8_values[:,:,3].reshape(-1,1)\n","inp_array_scaled = field_scaler.fit_transform(inp_array)\n","eng_feat_preprocessors.update({'DAYS_CREDIT_MEAN':field_scaler})\n","inp_array_scaled = np.expand_dims(inp_array_scaled, axis=2)\n","input8_values_scaled = np.append(input8_values_scaled,inp_array_scaled,axis=2)\n","\n","# scale TARGET mean of 500 neighbors\n","field_scaler = MinMaxScaler(feature_range=(1e-3, 1))  \n","inp_array = input8_values[:,:,4].reshape(-1,1)\n","inp_array_scaled = field_scaler.fit_transform(inp_array)\n","eng_feat_preprocessors.update({'TARGET_MEAN_500':field_scaler})\n","inp_array_scaled = np.expand_dims(inp_array_scaled, axis=2)\n","input8_values_scaled = np.append(input8_values_scaled,inp_array_scaled,axis=2)\n","\n","np.save(\"final_data/input8_values_scaled\",input8_values_scaled)\n","\n","eng_feat_preprocessors_file = open('preprocessors/eng_feat_preprocessors','wb')\n","pickle.dump(eng_feat_preprocessors,eng_feat_preprocessors_file)\n","eng_feat_preprocessors_file.close()"],"execution_count":null,"outputs":[]}]}